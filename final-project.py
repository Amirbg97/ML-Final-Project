import numpy
import matplotlib.pyplot as plt
import pandas
import math
import zipfile
# temporary fix for warning generated by dataframe.replace (line 16)
import warnings
warnings.simplefilter("ignore", category=FutureWarning)


def preprocess(X):
	# drop ID
	X = X.drop(X.columns[0], axis=1)

	# make targets 0,1
	X = X.replace({'B': 0, 'M': 1}).infer_objects(copy=False)

	return X
  
def logistic_regression(X_training, X_validation, Y_training, Y_validation
						,eta=.00004, num_epochs=100000, plot=False, returnPredictions=False):
	X = pandas.concat([X_training, X_validation])
	Y = pandas.concat([Y_training, Y_validation])

	w = numpy.random.rand(X_training.shape[1], 1) 
	X_training = numpy.array(X_training) # X = NxD
	Y_training = numpy.array(Y_training).reshape(-1, 1)

	
	eps = 10e-7 # to ensure no dividing by 0, log(0)
	
	# update weights and mean log loss at each step
	training_logloss = [0]*num_epochs
	validation_logloss = [0]*num_epochs
	for i in range(num_epochs):

		# numpy.exp can overflow with too many epochs/high eta
		try:
			w += eta*1/len(X_training)*X_training.T@(Y_training - 1/(1+numpy.exp(-X_training@w)))
		except:
			break
		# sorry these lines are gross...	
		training_logloss[i] = numpy.mean(- (Y_training*numpy.log(1/(1+numpy.exp(-X_training@w))+eps)+ (1-Y_training)*numpy.log(1-1/(1+numpy.exp(-X_training@w))+eps)))
		validation_logloss[i] = numpy.mean(- (Y_validation*numpy.log(1/(1+numpy.exp(-X_validation@w))+eps)+ (1-Y_validation)*numpy.log(1-1/(1+numpy.exp(-X_validation@w))+eps)))
		
	# classify observations as spam or not depending on whether g(x) >=.5; use astype(int) to make 1/0 instead of true/false	
	training_classifications = (1 / (1 + numpy.exp(-X_training @ w)) >= 0.5).astype(int)
	validation_classifications = (1 / (1 + numpy.exp(-X_validation @ w)) >= 0.5).astype(int)


	Y_validation = numpy.array(Y_validation).reshape(-1, 1)
	
	# calculate number of true positives, true negatives, false positives, false negatives
	training_TP = numpy.sum((training_classifications==1) & (Y_training==1), axis=0)
	training_TN = numpy.sum((training_classifications==0) & (Y_training==0), axis=0)
	training_FP = numpy.sum((training_classifications==1) & (Y_training==0), axis=0)
	training_FN = numpy.sum((training_classifications==0) & (Y_training==1), axis=0)

	validation_TP = numpy.sum((validation_classifications==1) & (Y_validation==1), axis=0)
	validation_TN = numpy.sum((validation_classifications==0) & (Y_validation==0), axis=0)
	validation_FP = numpy.sum((validation_classifications==1) & (Y_validation==0), axis=0)
	validation_FN = numpy.sum((validation_classifications==0) & (Y_validation==1), axis=0)


	# training stats
	training_accuracy = (training_TP+training_TN)/len(X_training)
	validation_accuracy = (validation_TP+validation_TN)/len(X_validation)
	validation_precision = validation_TP/(validation_TP+validation_FP)
	validation_recall = validation_TP/(validation_TP+validation_FN)
	validation_fmeasure = 2*validation_precision*validation_recall/(validation_precision+validation_recall)

	prior = numpy.sum(Y==1, axis=0)/len(Y)
	

	# plotting; blue is training log loss, red is validation
	if plot:
		plt.plot(list(range(1, num_epochs + 1)), training_logloss,  color='b')
		plt.plot(validation_logloss, color='r')
		plt.show()  
		
	
	if returnPredictions:

		training_classifications = (1 / (1 + numpy.exp(-X_training @ w)))
		validation_classifications = (1 / (1 + numpy.exp(-X_validation @ w))).to_numpy()
		

		return {
		'training classifications': training_classifications,	
		'validation classifications': validation_classifications,
		'weights': w,
		'training accuracy': training_accuracy,
		'validation accuracy': validation_accuracy,
		'validation precision': validation_precision,
		'validation recall': validation_recall,
		'validation fmeasure': validation_fmeasure
		}

	return {
		'training accuracy': training_accuracy,
		'validation accuracy': validation_accuracy,
		'validation precision': validation_precision,
		'validation recall': validation_recall,
		'validation fmeasure': validation_fmeasure
		}

def SVM(X_training, X_validation, Y_training, Y_validation, kernel='rbf', returnPredictions=False):
	X = pandas.concat([X_training, X_validation])
	Y = pandas.concat([Y_training, Y_validation])

	# switch values to {-1,1} for SVM
	Y = Y.replace(0, -1)
	Y_training = Y_training.replace(0, -1)
	Y_validation = Y_validation.replace(0, -1)
	
	# kernel
	def K(a,b,p=15,sigma=2):
		a = numpy.array(a)
		b = numpy.array(b)
			
		if kernel=='linear':
			return a@b.T
		elif kernel=='polynomial':
			if p<=1:
				print('degree of polynomial must be greater than 1')
				quit()
			return (a@b.T+1)**p
		elif kernel=='rbf':
			# this does not work if a and b are not the same size
			#return numpy.exp(-((a-b)@(a-b).T)/(2*sigma**2)) 

			# based on a stack overflow post
			numerator = numpy.sum(a**2, axis=1)[:, None]+numpy.sum(b**2, axis=1)-2*a@b.T
			return numpy.exp(-numerator/(2*sigma**2))
		else:
			print(kernel + ' is not a valid kernel. valid kernels = {linear, polynomial, rbf}')
			quit()

	# lagrange constraints
	alpha = numpy.linalg.pinv(numpy.diag(Y_training)@K(X_training,X_training)@numpy.diag(Y_training))@numpy.ones(len(X_training))
	
	# data * weights
	#g = K(X,X_training)@numpy.diag(Y_training)@alpha

	training_classifications = (K(X_training,X_training)@numpy.diag(Y_training)@alpha >= 0).astype(int)
	validation_classifications = (K(X_validation,X_training)@numpy.diag(Y_training)@alpha >= 0).astype(int)
	Y = Y.values.reshape(-1,1)
	
	# calculate number of true positives, true negatives, false positives, false negatives
	training_TP = numpy.sum((training_classifications==1) & (Y_training==1), axis=0)
	training_TN = numpy.sum((training_classifications==0) & (Y_training==-1), axis=0)
	training_FP = numpy.sum((training_classifications==1) & (Y_training==-1), axis=0)
	training_FN = numpy.sum((training_classifications==0) & (Y_training==1), axis=0)

	validation_TP = numpy.sum((validation_classifications==1) & (Y_validation==1), axis=0)
	validation_TN = numpy.sum((validation_classifications==0) & (Y_validation==-1), axis=0)
	validation_FP = numpy.sum((validation_classifications==1) & (Y_validation==-1), axis=0)
	validation_FN = numpy.sum((validation_classifications==0) & (Y_validation==1), axis=0)


	# training stats
	training_accuracy = (training_TP+training_TN)/len(X_training)
	validation_accuracy = (validation_TP+validation_TN)/len(X_validation)
	validation_precision = validation_TP/(validation_TP+validation_FP)
	validation_recall = validation_TP/(validation_TP+validation_FN)
	validation_fmeasure = 2*validation_precision*validation_recall/(validation_precision+validation_recall)

	prior = numpy.sum(Y==1, axis=0)/len(Y)

	
	if returnPredictions:
		training_classifications = K(X_training, X_training) @ numpy.diag(Y_training) @ alpha
		validation_classifications = K(X_validation, X_training) @ numpy.diag(Y_training) @ alpha
		

		return {
		'training classifications': training_classifications,
		'validation classifications': validation_classifications,	
		'training accuracy': training_accuracy,
		'validation accuracy': validation_accuracy,
		'validation precision': validation_precision,
		'validation recall': validation_recall,
		'validation fmeasure': validation_fmeasure
		}
	
	return {
		'training accuracy': training_accuracy,
		'validation accuracy': validation_accuracy,
		'validation precision': validation_precision,
		'validation recall': validation_recall,
		'validation fmeasure': validation_fmeasure
		}


def ensemble(X, k=5, oversample=False):
	# AdABoost (based on slides)
	M = len(X) // k

	metrics = {'training accuracy': [], 'validation accuracy': [], 'validation precision': [], 'validation recall': [], 'validation fmeasure': []}

	Y = X.iloc[:, 0]
	X = X.iloc[:, 1:]

	# z-score
	X = (X-numpy.atleast_2d(numpy.mean(X,axis=0)))/numpy.atleast_2d(numpy.std(X,axis=0)) # z-score

	# add bias feature (column)
	X['bias'] = 1

	probs = numpy.ones(len(X))	

	# if oversampling, pick minority class with higher prob
	if oversample:
		# minority class is M (1) ~ 37%
		X_minority = X[Y == 1]
		p = len(X)-len(X_minority)
		for i in range(len(X)):
			if Y[i] == 1:
				probs[i] = p
			else:
				probs[i] = len(X_minority)
			

	probs /= numpy.sum(probs)
	
	training_indices = numpy.random.choice(len(X), size=M, replace=False, p=probs)

	# validation is rest
	validation_indices = list(set(range(len(X))) - set(training_indices))

	
	X_training = X.iloc[training_indices].reset_index(drop=True)
	Y_training = Y.iloc[training_indices].reset_index(drop=True)

	
	X_validation = X.iloc[validation_indices].reset_index(drop=True)
	Y_validation = Y.iloc[validation_indices].reset_index(drop=True)
		
	
	# get logistic regression predictions and stats
	lr_vals = logistic_regression(X_training, X_validation, Y_training, Y_validation, returnPredictions=True)
	training_classifications = lr_vals['training classifications']
	lr_weights = lr_vals['weights']
	
	# for resampling
	eps = 1e-7
	misclassified_1 = (training_classifications.flatten() != Y_training)
	probs_training = probs[training_indices]
	error_rate_1 = numpy.sum(probs_training[misclassified_1])
	beta_1 = error_rate_1 / (1 - error_rate_1 + eps)
	alpha_1 = numpy.log(1 / beta_1 + eps)
	
	for i, classification in zip(X_training.index, training_classifications):
		if classification == Y.iloc[i]:
			probs[i] *= beta_1.item()

	probs /= numpy.sum(probs)

	
	# determine training set for svm
	training_indices = numpy.random.choice(len(X), size=M, replace=False, p=probs)

	# validation is rest
	validation_indices = list(set(range(len(X))) - set(training_indices))

	
	X_training_resampled = X.iloc[training_indices].reset_index(drop=True)
	Y_training_resampled = Y.iloc[training_indices].reset_index(drop=True)

	
	X_validation_resampled = X.iloc[validation_indices].reset_index(drop=True)
	Y_validation_resampled = Y.iloc[validation_indices].reset_index(drop=True)

	# get svm predictions and stats
	svm_vals = SVM(X_training_resampled, X_validation_resampled, Y_training_resampled, Y_validation_resampled, returnPredictions=True)

	misclassified_2 = (svm_vals['training classifications'] != Y_training_resampled)
	probs_training = probs[training_indices]
	error_rate_2 = numpy.sum(probs_training[misclassified_2])
	beta_2 = error_rate_1 / (1 - error_rate_2 + eps)
	alpha_2 = numpy.log(1 / beta_2 + eps)
		
	# multiply LR weights by SVM validation set for final predictions	
	LR_predictions = (1 / (1 + numpy.exp(-X_validation_resampled @ lr_weights))).to_numpy()
	# convert LR predictions from [0,1] to [-1,1] to combine with svm
	LR_predictions = 2 * LR_predictions - 1
	SVM_predictions = svm_vals['validation classifications']
	
	LR_predictions = LR_predictions.flatten()
	SVM_predictions = SVM_predictions.flatten()

	# weighted predictions
	final_classifications = numpy.sign(alpha_1 * LR_predictions + alpha_2 * SVM_predictions)

	
	# compute true positive, true negatives, etc.
	Y_validation_resampled = Y_validation_resampled.replace(0, -1)
	num_TP = numpy.sum((final_classifications==1) & (Y_validation_resampled==1), axis=0)
	num_TN = numpy.sum((final_classifications==-1) & (Y_validation_resampled==-1), axis=0)
	num_FP = numpy.sum((final_classifications==1) & (Y_validation_resampled==-1), axis=0)
	num_FN = numpy.sum((final_classifications==-1) & (Y_validation_resampled==1), axis=0)


	# training stats
	accuracy = (num_TP+num_TN)/len(X_validation_resampled)
	precision = num_TP/(num_TP+num_FP)
	recall = num_TP/(num_TP+num_FN)
	fmeasure = 2*precision*recall/(precision+recall)

	return {
	'accuracy': accuracy,
	'precision': precision,
	'recall': recall,
	'fmeasure': fmeasure
	}




def cross_validation(X, model, k=5):
	fold_size = len(X) // k
	metrics = {'training accuracy': [], 'validation accuracy': [], 'validation precision': [], 'validation recall': [], 'validation fmeasure': []}

	X = X.sample(frac=1).reset_index(drop=True)
	# Separate data from targets
	Y = X.iloc[:, 0]
	X = X.iloc[:, 1:]

	# z-score
	X = (X-numpy.atleast_2d(numpy.mean(X,axis=0)))/numpy.atleast_2d(numpy.std(X,axis=0)) # z-score

	# add bias feature (column)
	X['bias'] = 1


	for i in range(k):

		val_start = i * fold_size
		val_end = (i + 1) * fold_size if i != k - 1 else len(X)  # last fold may have extra data
		X_validation = X[val_start:val_end]
		Y_validation = Y[val_start:val_end]

		# training is rest
		X_training = pandas.concat([X[:val_start], X[val_end:]])
		Y_training = pandas.concat([Y[:val_start], Y[val_end:]])


		if model == 'logistic regression': 
			metrics_fold = logistic_regression(X_training, X_validation, Y_training, Y_validation)
		elif model == 'svm':
			metrics_fold = SVM(X_training, X_validation, Y_training, Y_validation)	
		
		metrics['training accuracy'].append(metrics_fold['training accuracy'])
		metrics['validation accuracy'].append(metrics_fold['validation accuracy'])
		metrics['validation precision'].append(metrics_fold['validation precision'])
		metrics['validation recall'].append(metrics_fold['validation recall'])
		metrics['validation fmeasure'].append(metrics_fold['validation fmeasure'])

	mean_metrics = {key: numpy.mean(value) for key, value in metrics.items()}
	return mean_metrics


def main():
	
	# dataset from UCI Machine Learning Repository
	zip_path = "breast+cancer+wisconsin+diagnostic.zip"

	# read data from zip file
	with zipfile.ZipFile(zip_path, 'r') as zip_ref:
		with zip_ref.open('wdbc.data') as file:
			X = pandas.read_csv(file, header=None)
		
	X = preprocess(X)
	
	# run both classifiers using cross validation
	
	LR_mean_metrics = cross_validation(X, 'logistic regression')
	print('logistic regression metrics:')
	for metric, value in LR_mean_metrics.items():
		print(f"{metric}: {value:.4f}")
	
		
	SVM_mean_metrics = cross_validation(X, 'svm')
	print('\nsvm metrics:')
	for metric, value in SVM_mean_metrics.items():
		print(f"{metric}: {value:.4f}")
	
	
	# use AdaBoost ensemble	on classifiers

	ensemble_metrics = ensemble(X)
	print('\nensemble metrics:')
	for metric, value in ensemble_metrics.items():
		print(f"{metric}: {value:.4f}")

if __name__=="__main__":
    main()